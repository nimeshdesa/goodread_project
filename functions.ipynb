{"cells":[{"cell_type":"code","source":"pip install bs4","metadata":{"tags":[],"cell_id":"00000-70ab5455-74ca-4960-9174-4a7fee643334","deepnote_to_be_reexecuted":false,"source_hash":"aa934e99","execution_start":1610563777295,"execution_millis":3052,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\nCollecting bs4\n  Downloading bs4-0.0.1.tar.gz (1.1 kB)\nCollecting beautifulsoup4\n  Downloading beautifulsoup4-4.9.3-py3-none-any.whl (115 kB)\n\u001b[K     |████████████████████████████████| 115 kB 16.1 MB/s \n\u001b[?25hCollecting soupsieve>1.2; python_version >= \"3.0\"\n  Downloading soupsieve-2.1-py3-none-any.whl (32 kB)\nBuilding wheels for collected packages: bs4\n  Building wheel for bs4 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1273 sha256=29ac10c7b7c29266b55a62024ccf5b26dfac24eec2e0bad2868c19a79b5ef885\n  Stored in directory: /tmp/pip-ephem-wheel-cache-6i01a1d4/wheels/0a/9e/ba/20e5bbc1afef3a491f0b3bb74d508f99403aabe76eda2167ca\nSuccessfully built bs4\nInstalling collected packages: soupsieve, beautifulsoup4, bs4\nSuccessfully installed beautifulsoup4-4.9.3 bs4-0.0.1 soupsieve-2.1\n\u001b[33mWARNING: You are using pip version 20.2.4; however, version 20.3.3 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00000-50938eac-7f59-4b6c-bf2c-ceee01f9809b","deepnote_to_be_reexecuted":false,"source_hash":"aaa7f29f","execution_millis":17,"execution_start":1610563821442,"deepnote_cell_type":"code"},"source":"#this cell is mine\nimport requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup\ndef scraper():\n    urls = []\n    author_list= []\n    avg_rating_list = []\n    award_list = []\n    genre_list = []\n    number_of_ratings_list = []\n    number_of_reviews_list = []\n    page_list = []\n    places_list = []\n    publish_year_list = []\n    series_list = []\n    titles_list = []\n    k=1\n    while len(urls) < 1001:\n        url_code = requests.get(f'https://www.goodreads.com/list/show/47.Best_Dystopian_and_Post_Apocalyptic_Fiction?page={k}')\n        url_raw = BeautifulSoup(url_code.content, 'html.parser')).find_all('a')[132:1124:10]\n         for i, code in enumerate(url_raw)\n            urls.append('https://www.goodreads.com/'+str(code).split('>')[0].split(' ')[1].split('\"')[1])\n        k+=1\n    del urls[1000:-1]\n    for j, link in enumerate(urls):\n        data_code = requests.get(link[0].strip(\"[']\"))\n        data = BeautifulSoup(data_code.content, 'html.parser')\n        try:\n            author = data.find('span', itemprop='name').get_text()\n            author_list.append(author)\n            avg_rating = float(data.find('span', itemprop='ratingValue').get_text().strip('\\n').split(' '))\n            avg_rating_list.append(avg_rating)\n            awards = [award.get_text() for k, award in enumerate(data.find_all('a', class_='award'))]\n            award_list.append(awards)\n            genres = data.find_all('a', class_='actionLinkLite bookPageGenreLink')\n            genre_list.append([all_genres[0:3][l].get_text() for l in range(3)])\n            number_of_ratings = int(''.join(data.find('meta', itemprop='ratingCount').get_text().strip('\\n ')\n            number_of_ratings_list.append(number_of_ratings.strip('\\n ratings').split(','))))\n            number_of_reviews = int(''.join(data.find('meta', itemprop='reviewCount').get_text().strip('\\n ')\n            number_of_reviews_list.append(number_of_ratings.strip('\\n reviews').split(','))))\n            page = data.find('span', itemprop='numberOfPages')\n            page_list.append(page.get_text().strip(' pages'))\n            places = [data.find_all('a').split('/')[1] == 'places']\n            places_list.append(', '.join([place.get_text() for m, place in enumerate(places)])\n            publish_year = data.find_all('div', class_='row')[1].get_text().strip('\\n').split(' ')\n            publish_year_list.append(item.strip('\\n') for n, item in enumerate(publish_year) if item.strip('\\n').isnumeric())\n            title = data.find('h1', class_='gr-h1 gr-h1--serif')\n            titles_list.append(title.get_text().strip('\\n').strip(' '))\n            series = data.find('a', class_='greyText')\n            series_list.append(1 if series.get_text().strip('\\n').strip(' ') == 'Edit Details' else 0)\n        except (AttributeError, IndexError):\n            author_list.append('')\n            avg_rating_list.append('')\n            award_list.append('')\n            genre_list.append('')\n            number_of_ratings_list.append('')\n            number_of_reviews_list.append('')\n            page_list.append('')\n            place_list.append('')\n            publish_year_list.append('')\n            titles_list.append('')\n            series_list.append('')\n    Data = pd.DataFrame(data={'URL': urls,\n                              'Title': titles_list,\n                              'Author': author_list,\n                              'Number of reviews': number_of_reviews_list,\n                              'Number of ratings': number_of_ratings_list,\n                              'Average rating': avg_rating_list,\n                              'Pages': page_list,\n                              'Publish year': publish_year_list,\n                              'Series': series_list,\n                              'Genres': genre_list,\n                              'Awards': award_list,\n                              'Places': place_list})\n    Data.to_csv('Data.csv')","outputs":[],"execution_count":null},{"cell_type":"code","source":"#preprocessing\ndef tran_awards(x): \n    return len(x)\n\n\ndf[\"Awards\"] = df[\"Awards\"].apply(tran_awards)\ndf['Awards'].isnull.sum()\n","metadata":{"tags":[],"cell_id":"00001-6f2e3ecd-aca5-465b-893d-3bf38677eb00","deepnote_to_be_reexecuted":false,"source_hash":"63d2e44d","execution_millis":0,"execution_start":1610553613601,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#analyse\n## Analysis Part 1.\ndef Groupby(df):\n    bygroup_minmax_df = pd.DataFrame(data=df.groupby('Publish year')['minmax_norm_rating'].agg('mean'))\n    return bygroup_minmax_df\n\n## Analysis part 2.\ndef Get_Info(authorname, df):\n    author_data = df[df['Author']==authorname]\n    author_info = author_data[author_data.minmax_norm_rating == author_data.minmax_norm_rating.max()]\n    return author_info","metadata":{"tags":[],"cell_id":"00002-6b33e53b-a7aa-495f-9bda-ca43646f007c","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#main func","metadata":{"tags":[],"cell_id":"00003-22af7f49-5da0-4c1d-a67d-61ff2df94c30","deepnote_cell_type":"code"},"outputs":[],"execution_count":null}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"d5b1bf40-ef94-4d5d-b21b-04f85edc669e","deepnote_execution_queue":[]}}